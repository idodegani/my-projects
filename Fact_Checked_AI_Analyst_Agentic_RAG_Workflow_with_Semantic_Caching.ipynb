{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vnaG2JCj_KJ"
      },
      "source": [
        "# The Fact-Checked Analyst: Agentic RAG Workflow with Semantic Caching\n",
        "\n",
        "## Project Overview\n",
        "I built an agentic workflow with LangGraph that combines RAG techniques with semantic caching and embedding reuse.  \n",
        "The system orchestrates multiple specialized agents responsible for retrieval, fact-checking, and generating responses, ensuring efficient and reliable analysis.\n",
        "\n",
        "---\n",
        "\n",
        "## Core Innovation: Semantic Cache Layer\n",
        "The semantic cache detects when similar questions are asked, even with different phrasing.  \n",
        "This reduces redundant processing and speeds up response times while keeping answers consistent.\n",
        "\n",
        "---\n",
        "\n",
        "## Technical Architecture Highlights\n",
        "- Multi-agent workflow in LangGraph managing retrieval, writing, and verification  \n",
        "- Semantic chunking to maintain context across document sections  \n",
        "- Embedding-based caching with similarity thresholds for prompt reuse  \n",
        "- Token-aware processing for cost and performance optimization  \n",
        "\n",
        "---\n",
        "\n",
        "## Real-World Application\n",
        "Applied to Apple's 10-K report, the system extracts key insights on risks, market trends, and strategy.  \n",
        "It scales across document collections while maintaining fast response times for cached queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22Qi03s-lYw3",
        "outputId": "a8900051-b96e-45fa-ef29-35d5d63ee416"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Environment configured successfully\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# API Configuration\n",
        "os.environ['OPENAI_API_KEY'] = 'sk-'\n",
        "\n",
        "print(\"✅ Environment configured successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6AvbolpkGJ7",
        "outputId": "bcff30bc-a32d-4328-b3f8-609c4fe9a289"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langgraph==0.2.34 in /usr/local/lib/python3.11/dist-packages (0.2.34)\n",
            "Requirement already satisfied: langchain==0.3.7 in /usr/local/lib/python3.11/dist-packages (0.3.7)\n",
            "Requirement already satisfied: langchain-openai==0.2.9 in /usr/local/lib/python3.11/dist-packages (0.2.9)\n",
            "Requirement already satisfied: langchain-community==0.3.7 in /usr/local/lib/python3.11/dist-packages (0.3.7)\n",
            "Requirement already satisfied: pypdf==5.1.0 in /usr/local/lib/python3.11/dist-packages (5.1.0)\n",
            "Requirement already satisfied: pandas==2.2.3 in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn==1.5.2 in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: tiktoken==0.8.0 in /usr/local/lib/python3.11/dist-packages (0.8.0)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.2.39 in /usr/local/lib/python3.11/dist-packages (from langgraph==0.2.34) (0.3.63)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from langgraph==0.2.34) (2.1.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.7) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.7) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.7) (3.12.15)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.7) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.7) (0.1.147)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.7) (2.11.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.7) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.7) (8.5.0)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.54.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.2.9) (1.99.1)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.7) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.7) (0.4.1)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.7) (2.10.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.3) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (3.6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken==0.8.0) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.7) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.7) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.2.39->langgraph==0.2.34) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.2.39->langgraph==0.2.34) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.2.39->langgraph==0.2.34) (4.14.1)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.0->langgraph==0.2.34) (1.10.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.7) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.7) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.7) (1.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai==0.2.9) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai==0.2.9) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai==0.2.9) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai==0.2.9) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai==0.2.9) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.7) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.7) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.7) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.7) (1.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.3) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.7) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.7) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.7) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.7) (2025.8.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.7) (3.2.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.7) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.7) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.2.39->langgraph==0.2.34) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.7) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "# Install Required Dependencies\n",
        "!pip install langgraph==0.2.34 langchain==0.3.7 langchain-openai==0.2.9 langchain-community==0.3.7 pypdf==5.1.0 pandas==2.2.3 numpy==1.26.4 scikit-learn==1.5.2 tiktoken==0.8.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARJNtkftkHiX"
      },
      "source": [
        "# Environment Setup & Data Acquisition\n",
        "\n",
        "## Configuring API Access and Document Retrieval\n",
        "The system uses OpenAI's embedding and language models for vector search and natural language generation.  \n",
        "Apple’s latest 10-K report is downloaded directly for analysis and processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vybUHz0kB2k",
        "outputId": "7adfcbd2-1c24-4e88-a0ed-961129032990"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Environment configured successfully\n",
            "\n",
            "📄 Checking for local PDF file: apple_2024_k-10_report.pdf\n",
            "⚠️  apple_2024_k-10_report.pdf not found.\n",
            "Please make sure the file is in the same directory as your script or provide the full path.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict, Optional, TypedDict\n",
        "import json\n",
        "import tiktoken\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# API Configuration (optional, can be removed if not needed for subsequent steps)\n",
        "# os.environ['OPENAI_API_KEY'] = 'sk-...'\n",
        "# Note: It's best practice not to hardcode API keys.\n",
        "\n",
        "print(\"✅ Environment configured successfully\")\n",
        "\n",
        "# Define the local PDF file path\n",
        "file_path = \"apple_2024_k-10_report.pdf\"\n",
        "print(f\"\\n📄 Checking for local PDF file: {file_path}\")\n",
        "\n",
        "# Verify the file exists in the environment\n",
        "if os.path.exists(file_path):\n",
        "    # Calculate and print file size for confirmation\n",
        "    file_size = os.path.getsize(file_path) / (1024 * 1024)\n",
        "    print(f\"✅ {file_path} found ({file_size:.1f} MB). Ready to be loaded.\")\n",
        "else:\n",
        "    print(f\"⚠️  {file_path} not found.\")\n",
        "    print(\"Please make sure the file is in the same directory as your script or provide the full path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6N-Kp_6k6qA"
      },
      "source": [
        "# Part 1: Document Processing & Vector Database Creation\n",
        "\n",
        "## Building the Knowledge Base\n",
        "The foundation of the RAG system is a vector database built by processing the PDF, splitting it into semantic chunks, generating embeddings, and storing everything in a clear, structured format.  \n",
        "This setup enables efficient retrieval and full transparency into how the system works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rale8WgikKEe",
        "outputId": "b63a0cb0-58f2-4f6f-a04f-57c2137f4e39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "🔧 INITIALIZING DOCUMENT PROCESSING PIPELINE\n",
            "============================================================\n",
            "\n",
            "📄 Loading PDF document...\n",
            "✅ Loaded 121 pages\n",
            "\n",
            "✂️ Splitting document into semantic chunks...\n",
            "✅ Created 543 text chunks\n",
            "\n",
            "🧠 Initializing embedding model...\n",
            "\n",
            "⚡ Processing chunks (embedding + token counting)...\n",
            "   Processing chunk 0/543...\n",
            "   Processing chunk 50/543...\n",
            "   Processing chunk 100/543...\n",
            "   Processing chunk 150/543...\n",
            "   Processing chunk 200/543...\n",
            "   Processing chunk 250/543...\n",
            "   Processing chunk 300/543...\n",
            "   Processing chunk 350/543...\n",
            "   Processing chunk 400/543...\n",
            "   Processing chunk 450/543...\n",
            "   Processing chunk 500/543...\n",
            "\n",
            "✅ Database created with 543 chunks\n",
            "📊 Statistics:\n",
            "   - Average chunk size: 189.5 tokens\n",
            "   - Total tokens: 102,889\n",
            "   - Database saved to: document_chunks.csv\n",
            "\n",
            "📋 Sample Database Entries:\n",
            "============================================================\n",
            "\n",
            "Chunk 1:\n",
            "  Text preview: UNITED STATES\n",
            "SECURITIES AND EXCHANGE COMMISSION\n",
            "Washington, D.C. 20549\n",
            "FORM 10-K\n",
            "(Mark One)\n",
            "☒    AN...\n",
            "  Token count: 267\n",
            "  Embedding dims: 1536\n",
            "\n",
            "Chunk 2:\n",
            "  Text preview: Title of each class\n",
            "Trading \n",
            "symbol(s) Name of each exchange on which registered\n",
            "Common Stock, $0.00...\n",
            "  Token count: 265\n",
            "  Embedding dims: 1536\n",
            "\n",
            "Chunk 3:\n",
            "  Text preview: Indicate by check mark whether the Registrant (1) has filed all reports required to be filed by Sect...\n",
            "  Token count: 229\n",
            "  Embedding dims: 1536\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "import ast\n",
        "\n",
        "def setup_database(pdf_path: str = \"Apple_K10.pdf\",\n",
        "                  chunk_size: int = 1000,\n",
        "                  chunk_overlap: int = 200) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Processes a PDF document into a searchable vector database.\n",
        "\n",
        "    This function performs semantic chunking, embedding generation, and token counting\n",
        "    to create a comprehensive knowledge base stored in CSV format for transparency.\n",
        "\n",
        "    Args:\n",
        "        pdf_path: Path to the PDF document\n",
        "        chunk_size: Target size for text chunks in characters\n",
        "        chunk_overlap: Overlap between consecutive chunks to preserve context\n",
        "\n",
        "    Returns:\n",
        "        DataFrame containing processed chunks with embeddings and metadata\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"🔧 INITIALIZING DOCUMENT PROCESSING PIPELINE\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    # Load the PDF document\n",
        "    print(\"📄 Loading PDF document...\")\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    documents = loader.load()\n",
        "    print(f\"✅ Loaded {len(documents)} pages\")\n",
        "\n",
        "    # Initialize the text splitter with semantic awareness\n",
        "    print(\"\\n✂️ Splitting document into semantic chunks...\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]  # Hierarchical splitting\n",
        "    )\n",
        "\n",
        "    # Split documents into chunks\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    print(f\"✅ Created {len(chunks)} text chunks\")\n",
        "\n",
        "    # Initialize embedding model\n",
        "    print(\"\\n🧠 Initializing embedding model...\")\n",
        "    embeddings_model = OpenAIEmbeddings(\n",
        "        model=\"text-embedding-3-small\",\n",
        "        dimensions=1536\n",
        "    )\n",
        "\n",
        "    # Initialize tokenizer for accurate token counting\n",
        "    encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
        "\n",
        "    # Process each chunk\n",
        "    print(\"\\n⚡ Processing chunks (embedding + token counting)...\")\n",
        "    chunk_data = []\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        if i % 50 == 0:\n",
        "            print(f\"   Processing chunk {i}/{len(chunks)}...\")\n",
        "\n",
        "        # Extract text\n",
        "        chunk_text = chunk.page_content.strip()\n",
        "\n",
        "        # Generate embedding\n",
        "        chunk_embedding = embeddings_model.embed_query(chunk_text)\n",
        "\n",
        "        # Count tokens\n",
        "        token_count = len(encoding.encode(chunk_text))\n",
        "\n",
        "        # Store as dictionary\n",
        "        chunk_data.append({\n",
        "            'chunk_text': chunk_text,\n",
        "            'chunk_embedding': str(chunk_embedding),  # Store as string for CSV\n",
        "            'token_count': token_count\n",
        "        })\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(chunk_data)\n",
        "\n",
        "    # Save to CSV\n",
        "    csv_path = 'document_chunks.csv'\n",
        "    df.to_csv(csv_path, index=False)\n",
        "\n",
        "    print(f\"\\n✅ Database created with {len(df)} chunks\")\n",
        "    print(f\"📊 Statistics:\")\n",
        "    print(f\"   - Average chunk size: {df['token_count'].mean():.1f} tokens\")\n",
        "    print(f\"   - Total tokens: {df['token_count'].sum():,}\")\n",
        "    print(f\"   - Database saved to: {csv_path}\")\n",
        "\n",
        "    # Display sample entries\n",
        "    print(f\"\\n📋 Sample Database Entries:\")\n",
        "    print(f\"{'='*60}\")\n",
        "    for i in range(min(3, len(df))):\n",
        "        print(f\"\\nChunk {i+1}:\")\n",
        "        print(f\"  Text preview: {df.iloc[i]['chunk_text'][:100]}...\")\n",
        "        print(f\"  Token count: {df.iloc[i]['token_count']}\")\n",
        "        print(f\"  Embedding dims: {len(ast.literal_eval(df.iloc[i]['chunk_embedding']))}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Execute database setup\n",
        "document_db = setup_database()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSFJ42OYlCGN"
      },
      "source": [
        "# Part 2: Intelligent Semantic Caching System\n",
        "\n",
        "## Building the Cache Layer\n",
        "The semantic cache is key to the system’s speed and consistency.  \n",
        "It detects when similar questions have been asked before using embedding similarity, allowing instant responses even if the queries are phrased differently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0gaE4rYck8dp",
        "outputId": "40c31a59-3edb-4b16-d30f-f95237d88516"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📦 Initialized new cache at prompt_cache.csv\n",
            "✅ Semantic cache system initialized\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import ast\n",
        "import os\n",
        "\n",
        "class SemanticCache:\n",
        "    \"\"\"\n",
        "    Implements an intelligent caching system based on semantic similarity.\n",
        "\n",
        "    This cache recognizes when similar questions have been asked before,\n",
        "    even if phrased differently, and returns cached results instantly.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cache_file: str = \"prompt_cache.csv\",\n",
        "                 similarity_threshold: float = 0.95):\n",
        "        \"\"\"\n",
        "        Initialize the semantic cache with configurable similarity threshold.\n",
        "\n",
        "        Args:\n",
        "            cache_file: Path to the CSV file storing cached prompts and answers\n",
        "            similarity_threshold: Minimum cosine similarity to trigger cache hit\n",
        "        \"\"\"\n",
        "        self.cache_file = cache_file\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.embeddings_model = OpenAIEmbeddings(\n",
        "            model=\"text-embedding-3-small\",\n",
        "            dimensions=1536\n",
        "        )\n",
        "\n",
        "        # Initialize cache file if it doesn't exist\n",
        "        if not os.path.exists(self.cache_file):\n",
        "            empty_cache = pd.DataFrame(columns=['prompt_text', 'prompt_embedding', 'cached_answer'])\n",
        "            empty_cache.to_csv(self.cache_file, index=False)\n",
        "            print(f\"📦 Initialized new cache at {self.cache_file}\")\n",
        "\n",
        "    def check_cache(self, user_query: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Checks if a similar query exists in the cache.\n",
        "\n",
        "        Uses cosine similarity between embeddings to find semantically similar queries.\n",
        "\n",
        "        Args:\n",
        "            user_query: The new query to check against the cache\n",
        "\n",
        "        Returns:\n",
        "            Cached answer if similarity exceeds threshold, None otherwise\n",
        "        \"\"\"\n",
        "\n",
        "        # Load existing cache\n",
        "        if not os.path.exists(self.cache_file):\n",
        "            return None\n",
        "\n",
        "        cache_df = pd.read_csv(self.cache_file)\n",
        "\n",
        "        if cache_df.empty:\n",
        "            return None\n",
        "\n",
        "        print(f\"\\n🔍 Checking semantic cache ({len(cache_df)} entries)...\")\n",
        "\n",
        "        # Embed the new query\n",
        "        query_embedding = self.embeddings_model.embed_query(user_query)\n",
        "        query_embedding_np = np.array(query_embedding).reshape(1, -1)\n",
        "\n",
        "        # Calculate similarities with all cached prompts\n",
        "        max_similarity = 0\n",
        "        best_match_idx = -1\n",
        "\n",
        "        for idx, row in cache_df.iterrows():\n",
        "            cached_embedding = np.array(ast.literal_eval(row['prompt_embedding'])).reshape(1, -1)\n",
        "            similarity = cosine_similarity(query_embedding_np, cached_embedding)[0][0]\n",
        "\n",
        "            if similarity > max_similarity:\n",
        "                max_similarity = similarity\n",
        "                best_match_idx = idx\n",
        "\n",
        "        print(f\"   Maximum similarity found: {max_similarity:.3f}\")\n",
        "\n",
        "        # Check if we have a cache hit\n",
        "        if max_similarity >= self.similarity_threshold:\n",
        "            cached_prompt = cache_df.iloc[best_match_idx]['prompt_text']\n",
        "            cached_answer = cache_df.iloc[best_match_idx]['cached_answer']\n",
        "\n",
        "            print(f\"\\n🎯 CACHE HIT! (Similarity: {max_similarity:.3f})\")\n",
        "            print(f\"   Original query: '{cached_prompt[:80]}...'\")\n",
        "            print(f\"   Your query:     '{user_query[:80]}...'\")\n",
        "\n",
        "            return cached_answer\n",
        "\n",
        "        print(f\"   No cache hit (threshold: {self.similarity_threshold})\")\n",
        "        return None\n",
        "\n",
        "    def update_cache(self, user_query: str, answer: str) -> None:\n",
        "        \"\"\"\n",
        "        Adds a new query-answer pair to the cache.\n",
        "\n",
        "        Args:\n",
        "            user_query: The original query\n",
        "            answer: The generated answer to cache\n",
        "        \"\"\"\n",
        "\n",
        "        # Generate embedding for the query\n",
        "        query_embedding = self.embeddings_model.embed_query(user_query)\n",
        "\n",
        "        # Load existing cache\n",
        "        if os.path.exists(self.cache_file):\n",
        "            cache_df = pd.read_csv(self.cache_file)\n",
        "        else:\n",
        "            cache_df = pd.DataFrame(columns=['prompt_text', 'prompt_embedding', 'cached_answer'])\n",
        "\n",
        "        # Add new entry\n",
        "        new_entry = pd.DataFrame([{\n",
        "            'prompt_text': user_query,\n",
        "            'prompt_embedding': str(query_embedding),\n",
        "            'cached_answer': answer\n",
        "        }])\n",
        "\n",
        "        cache_df = pd.concat([cache_df, new_entry], ignore_index=True)\n",
        "\n",
        "        # Save updated cache\n",
        "        cache_df.to_csv(self.cache_file, index=False)\n",
        "\n",
        "        print(f\"\\n💾 Cache updated (now {len(cache_df)} entries)\")\n",
        "\n",
        "# Initialize the semantic cache\n",
        "semantic_cache = SemanticCache(similarity_threshold=0.92)\n",
        "print(\"✅ Semantic cache system initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbwXSIUalG4q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}